# 139、为什么有了HDFS之后，还需要HBase呢？

hdfs，对hdfs是什么，功能，架构，源码，如何二次开发，都有了一定的理解，看懂hdfs那个课程的前提，Java架构部分的jdk集合、并发、IO、网络，自研分布式海量小文件系统的项目，最好去先做一下

 

hdfs设计主要是针对什么呢？针对的是大数据，超大文件，比如说你有一个超大文件，里面要放100GB的用户行为的日志，甚至是1TB，甚至1PB，针对一个网站的每天的用户行为的日志，可以都放一个超大文件里去

 

超大文件很难说放在一台服务器上，所以说此时，可以把超大文件拆散，拆成N多个128MB的小文件，每一个小文件就可以说是这个大文件的一个block

 

hdfs解决的主要是一个分布式存储的问题，也就是说你有超大数据集，不可能都放在一个文件里，是不现实的，所以可以拆分为N多个128MB的block小文件，分散存储在多台机器上，对超大数据集实现分布式存储的效果

 

每个block小文件还有3副本冗余存储，每个副本在不同的机器上，高可用和高容错，任何一台机器挂掉，不会导致数据丢失的

 

hdfs，hadoop distributed filesystem，分布式文件系统，他存放的是文件，文件死的，静态的，最多只能是你不停的往文件的末尾追加数据，他会把你追加到文件末尾的数据其实都是分散在不同的小block里存储在机器上

 

目录层级结构，创建文件，管理权限，对文件进行删除，大概就是这样的一些事情了，对大文件里的数据进行读取，对文件进行数据追加，hdfs只能做到如上一些事情

 

针对你hdfs上存储的海量数据，10TB的数据，我要进行增删改查，我要往里面插入数据，还要修改数据，还有删除里面某一行数据，还有精准的查询里面某一行数据，得了，hdfs上是大量的block小文件

 

虽然说帮你把超大数据集给分布式存储了，现实吗？根本就不现实

 

所以呢，当当当当，hbase出马了，由hbase基于hdfs进行超大数据集的分布式存储，让海量数据分布式存储在hdfs上，但是对hdfs里的海量数据进行精准的某一行，或者某几行的数据的增删改查，由hbase来解决了

 

hadoop nosql database